# Zookeeper

## ◆初识Zookeeper

### 1、Zookeeper概念

![image-20220629093508970](笔记图片资源包/image-20220629093508970.png)

`ZooKeeper`是一个集中的服务，用于维护配置信息、命名、提供分布式同步和提供组服务。所有这些类型的服务都以某种形式被分布式应用程序使用。每次它们被实现时，都会有大量的工作来修复不可避免的错误和竞争条件。由于实现这些服务的困难，应用程序最初通常会略过这些服务，这使得它们在出现更改时变得脆弱，并且难以管理。即使正确地执行了这些服务，在部署应用程序时，这些服务的不同实现也会导致管理复杂性

`zookeeper`由雅虎研究院开发,是` Google Chubby`的开源实现,后来托管到 `Apache`,于`2010年11月`正式成为`apache`的顶级项目

大数据生态系统里由很多组件的命名都是某些动物或者昆虫，比如`hadoop`大象，`hive`就是蜂巢，`zookeeper`即管理员，顾名思义就算管理大数据生态系统各组件的管理员，如下所示：

![zookeeper-1](笔记图片资源包/zookeeper-1.png)

#### 应用场景

`zookeepepr`是一个经典的**分布式**数据一致性解决方案，致力于为分布式应用提供一个高性能、高可用,且具有严格顺序访问控制能力的分布式协调存储服务。

- 维护配置信息
- 分布式锁服务
- 集群管理
- 生成分布式唯一ID

1. **维护配置信息**

   - `java`编程经常会遇到配置项，比如数据库的`url`、 `schema`、`user`和 `password`等。通常这些配置项我们会放置在配置文件中，再将配置文件放置在服务器上当需要更改配置项时，需要去服务器上修改对应的配置文件。

     但是随着分布式系统的兴起,由于许多服务都需要使用到该配置文件,因此有**必须保证该配置服务的高可用性**(`highavailability`)和各台服务器上配置数据的一致性。

     通常会将配置文件部署在一个集群上，然而一个**集群动辄上千台**服务器，此时如果再一台台服务器逐个修改配置文件那将是非常繁琐且危险的的操作，因此就**需要一种服务**，**能够高效快速且可靠地完成配置项的更改等操作**，并能够保证各配置项在每台服务器上的数据一致性。

     **`zookeeper`就可以提供这样一种服务**，其使用`Zab`这种一致性协议来保证一致性。现在有很多开源项目使用`zookeeper`来维护配置，如在 `hbase`中，客户端就是连接一个 `zookeeper`，获得必要的 `hbase`集群的配置信息，然后才可以进一步操作。还有在开源的消息队列 `kafka`中，也便用`zookeeper`来维护 `brokers`的信息。在 `alibaba`开源的`soa`框架`dubbo`中也广泛的使用`zookeeper`管理一些配置来实现服务治理。

     ![zookeeper-2](笔记图片资源包/zookeeper-2.png)

2. 分布式锁服务

   - 一个集群是一个分布式系统，由多台服务器组成。为了提高并发度和可靠性，多台服务器上运行着同一种服务。当多个服务在运行时就需要协调各服务的进度，有时候需要保证当某个服务在进行某个操作时，其他的服务都不能进行该操作，即对该操作进行加锁，如果当前机器挂掉后，释放锁并 `fail over`到其他的机器继续执行该服务

3. 集群管理

   - 一个集群有时会因为各种软硬件故障或者网络故障，出现棊些服务器挂掉而被移除集群，而某些服务器加入到集群中的情况，`zookeeper`会将这些服务器加入/移出的情况通知给集群中的其他正常工作的服务器，以及时调整存储和计算等任务的分配和执行等。此外`zookeeper`还会对故障的服务器做出诊断并尝试修复。

     ![zookeeper-3](笔记图片资源包/zookeeper-3.png)

4. 生产分布式唯一ID

   - 在过去的单库单表型系统中，通常可以使用数据库字段自带的`auto_ increment`属性来自动为每条记录生成一个唯一的`ID`。但是分库分表后，就无法在依靠数据库的`auto_ Increment`属性来唯一标识一条记录了。此时我们就可以用`zookeeper`在分布式环境下生成全局唯一`ID`。

     做法如下:每次要生成一个新`id`时，创建一个持久顺序节点，创建操作返回的节点序号，即为新`id`，然后把比自己节点小的删除即可

#### Zookeeper的设计目标

`zooKeeper`致力于为分布式应用提供一个高性能、高可用，且具有严格顺序访问控制能力的分布式协调服务

1. 高性能
   - `zookeeper`将全量数据存储在**内存**中，并直接服务于客户端的所有非事务请求，尤其用于以读为主的应用场景
2. 高可用
   - `zookeeper`一般以集群的方式对外提供服务，一般`3~5`台机器就可以组成一个可用的 `Zookeeper`集群了，每台机器都会在内存中维护当前的服务器状态，井且每台机器之间都相互保持着通信。只要集群中超过一半的机器都能够正常工作，那么整个集群就能够正常对外服务
3. 严格顺序访问
   - 对于来自客户端的每个更新请求，`Zookeeper`都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序



#### 数据模型

`zookeeper`的数据结点可以视为树状结构(或目录)，树中的各个结点被称为`znode `(即`zookeeper node`)，一个`znode`可以由多个子结点。`zookeeper`结点在结构上表现为树状；

使用路径`path`来定位某个`znode`，比如`/ns-1/itcast/mysqml/schemal1/table1`，此处`ns-1，itcast、mysql、schemal1、table1`分别是`根结点、2级结点、3级结点以及4级结点`；其中`ns-1`是`itcast`的父结点，`itcast`是`ns-1`的子结点，`itcast`是`mysql`的父结点....以此类推

`znode`，间距文件和目录两种特点，即像文件一样维护着数据、元信息、ACL、时间戳等数据结构，又像目录一样可以作为路径标识的一部分

![zookeeper-4](笔记图片资源包/zookeeper-4.png)

那么如何描述一个`znode`呢？一个`znode`大体上分为`3`个部分：

- 结点的数据：即`znode data `(结点`path`，结点`data`)的关系就像是`Java map `中的 `key value `关系
- 结点的子结点`children`
- 结点的状态`stat`：用来描述当前结点的创建、修改记录，包括`cZxid`、`ctime`等

#### 结点状态stat的属性

在`zookeeper shell `中使用 `get `命令查看指定路径结点的`data`、`stat`信息

![zookeeper-5](笔记图片资源包/zookeeper-5.png)

属性说明：

结点的各个属性如下。其中重要的概念是`Zxid(Zookeeper Transaction ID)`，`Zookeeper`结点的每一次更改都具有唯一的`Zxid`，如果`Zxid-1` 小于` Zxid-2` ，则`Zxid-1` 的更改发生在 `Zxid-2 `更改之前

<https://zookeeper.apache.org/doc/r3.4.14/zookeeperProgrammers.html#sc_zkDataModel_znodes>

- `cZxid`数据结点创建时的事务ID——针对于`zookeeper`数据结点的管理：我们对结点数据的一些写操作都会导致`zookeeper`自动地为我们去开启一个事务，并且自动地去为每一个事务维护一个事务`ID`
- `ctime`数据结点创建时的时间
- `mZxid`数据结点最后一次更新时的事务ID
- `mtime`数据结点最后一次更新时的时间
- `pZxid`数据节点最后一次修改此`znode`子节点更改的`zxid`
- `cversion`子结点的更改次数
- `dataVersion`结点数据的更改次数
- `aclVersion`结点的ACL更改次数——类似`linux`的权限列表，维护的是当前结点的权限列表被修改的次数
- `ephemeralOwner`如果结点是临时结点，则表示创建该结点的会话的`SessionID`；如果是持久结点，该属性值为0
- `dataLength`数据内容的长度
- `numChildren`数据结点当前的子结点个数

**结点类型**

`zookeeper`中的结点有两种，分别为**临时结点**和**永久结点**。结点的类型在创建时被确定，并且不能改变

- 临时节点：
  - 该节点的生命周期依赖于创建它们的会话。一旦会话( `Session`）结束，临时节点将被自动删除，当然可以也可以手动删除。虽然每个临时的 `Znode`都会绑定到一个客户端会话，但他们对所有的客户端还是可见的。另外，`Zookeeper`的临时节点不允许拥有子节点
- 持久化结点：
  - 该结点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，它们才能被删除

### 2、Zookeeper主要功能

#### 1、配置管理

如下图A、B、C可能都存在一些配置信息，当然便存在可能相同的配置信息，当相同的配置信息发生改变时，更改起来非常麻烦修改成本非常高，所以引入一个组件叫配置中心。

![image-20220629093640438](笔记图片资源包/image-20220629093640438.png)

让配置中心对A、B、C相同的配置信息进行统一管理，改起来也方便，降低了运维成本，A、B、C也不用储存这些相同的配置信息，当需要使用时，从配置中心拉取即可。

![image-20220629094001204](笔记图片资源包/image-20220629094001204.png)

#### 2、分布式锁

##### 单机锁

A服务要访问数据

![image-20220629094340986](笔记图片资源包/image-20220629094340986.png)

但A在访问的同时，被很多消费者访问

![image-20220629094447606](笔记图片资源包/image-20220629094447606.png)

此时为了保证访问数据的完整性和正确性，避免因为其它访问而导致的数据错乱，所以我们期望这些数据同一时间只能被一个人访问到，所以给访问加锁。以此保证访问数据的完整性和正确性

![image-20220629094857268](笔记图片资源包/image-20220629094857268.png)

##### 分布式锁

如果此时有个B服务，是另外一个应用程序，部署在另外一台服务器上，它也要访问这个数据，此时A中所加的锁对B是无效的，因为这个锁是JDK提供的，它跟JVM是强绑定的。而两台服务器有两个JVM。

![image-20220629095001872](笔记图片资源包/image-20220629095001872.png)

此时的锁便没了效果，所以不在A处加锁了，直接引入第三方锁(分布式锁)来控制访问，此时A如果要访问数据，必须先去分布式锁中获取数据的访问权限(钥匙)，然后分布式锁给予权限后A正常访问数据，与此同时，B也想去访问数据，但分布式锁中并没有空闲的数据访问权限(钥匙)，因为A还在访问数据，此时B只能等待A访问完数据才能有获取数据访问权限的资格，以此达到访问的唯一性。

![image-20220629095500651](笔记图片资源包/image-20220629095500651.png)

#### 3、集群管理

在提供者与消费者中作为注册中心使用，管理二者的交互

![image-20220629100006198](笔记图片资源包/image-20220629100006198.png)

## ◆ZooKeeper安装与配置

Dubbo官方推荐使用Zookeeper作为Registry(注册中心)

#### 1、下载安装

##### **1、环境准备**

ZooKeeper服务器是用Java创建的，它运行在JVM之上。需要安装JDK 7或更高版本。

##### **2、上传**

将下载的ZooKeeper放到/opt/ZooKeeper目录下

```shell
#上传zookeeper alt+p
put f:/setup/apache-zookeeper-3.5.6-bin.tar.gz
#打开 opt目录
cd /opt
#创建zooKeeper目录
mkdir  zooKeeper
#将zookeeper安装包移动到 /opt/zooKeeper
mv apache-zookeeper-3.5.6-bin.tar.gz /opt/zookeeper/
```

##### **3、解压**

将tar包解压到/opt/zookeeper目录下

```shell
tar -zxvf apache-zookeeper-3.5.6-bin.tar.gz 
```

#### 2、 配置启动

##### **1、配置zoo.cfg**

进入到conf目录拷贝一个zoo_sample.cfg并完成配置

```shell
#进入到conf目录
cd /opt/zookeeper/apache-zookeeper-3.5.6-bin/conf/
#拷贝
cp  zoo_sample.cfg  zoo.cfg
```

修改zoo.cfg

```shell
#打开目录
cd /opt/zookeeper/
#创建zooKeeper存储目录
mkdir  zkdata
#修改zoo.cfg
vim /opt/zookeeper/apache-zooKeeper-3.5.6-bin/conf/zoo.cfg
```

![1577548250377](笔记图片资源包/1577548250377.png)

修改存储目录：dataDir=/opt/zookeeper/zkdata

##### **2、启动ZooKeeper**

```shell
cd /opt/zookeeper/apache-zookeeper-3.5.6-bin/bin/
#启动
 ./zkServer.sh  start
```

![1577548052037](笔记图片资源包/1577548052037.png)

看到上图表示ZooKeeper成功启动

##### **3、查看ZooKeeper状态**

```shell
./zkServer.sh status
```

zookeeper启动成功。standalone代表zk没有搭建集群，现在是单节点

![1577548175232](笔记图片资源包/1577548175232.png)

zookeeper没有启动

![1577548112773](笔记图片资源包/1577548112773-16563885710711.png)

注意：连接时如果开了zookeeper但无法连接上时，记得关闭防火墙

1)查看防火墙状态：

```shell
service iptables status
```

2)开启防火墙：

```shell
service iptables start
```

3)关闭防火墙：

```shell
service iptables stop
```

4)重启防火墙：

```shell
service iptables restart
```

5)永久关闭防火墙：

```shell
chkconfig iptables off
```

6)永久关闭后重启：

```shell
chkconfig iptables on
```

Linux 7

```
//查看防火墙的状态的命令为：
sudo systemctl status firewalld
//打开防火墙的方式有两种，一种是打开后重启会恢复回原来的状态，命令为：
sudo systemctl start firewalld
//关闭的
sudo systemctl stop firewalld
/另一种是打开后重启不会恢复到原来的状态，命令为：这种方式输入命令后要重启系统才会生效。
sudo systemctl enable firewalld
//关闭
sudo systemctl disable firewalld
```



#### 3、zookeeper的Acl权限控制

<https://zookeeper.apache.org/doc/r3.4.14/zookeeperProgrammers.html#sc_ZooKeeperAccessControl>

`zookeeper `类似文件系统，`client`可以创建结点、更新结点、删除结点，那么如何做到结点的权限控制呢？

`zookeeper`的 `access control list` 访问控制列表可以做到这一点

`acl`权限控制，使用`scheme：id：permission `来标识，主要涵盖3个方面：

<https://zookeeper.apache.org/doc/r3.4.14/zookeeperProgrammers.html#sc_BuiltinACLSchemes>

- 权限模式(`scheme`)：授权的策略
- 授权对象(`id`)：授权的对象
- 权限(`permission`)：授予的权限

其特性如下：

- `zookeeper`的权限控制是基于每个`znode`结点的，需要对每个结点设置权限

- 每个`znode `支持多种权限控制方案和多个权限

- 子结点不会继承父结点的权限，客户端无权访问某结点，但可能可以访问它的子结点：

  例如`setAcl /test2 ip:192.168.133.133:crwda`  // 将结点权限设置为Ip：192.168.133.133 的客户端可以对节点进行
  增删改查和管理权限

##### **权限模式**

- 采用何种方式授权

- 

- | 方案   | 描述                                                    |
  | ------ | ------------------------------------------------------- |
  | world  | 只有一个用户：`anyone`，代表登录`zookeeper`所有人(默认) |
  | ip     | 对客户端使用IP地址认证                                  |
  | auth   | 使用已添加认证的用户认证                                |
  | digest | 使用"用户名：密码"方式认证                              |

##### **授权对象**

- 给谁授予权限
- 授权对象ID是指，权限赋予的实体，例如：IP地址或用户

##### **授权的权限**

- 授予什么权限

- `create、delete、read、writer、admin`也就是 增、删、查、改、管理权限，这5种权限简写为 c d r w a，注意：
  这五种权限中，有的权限并不是对结点自身操作的例如：delete是指对**子结点**的删除权限

  可以试图删除父结点，但是子结点必须删除干净，所以`delete`的权限也是很有用的

- 

- | 权限   | ACL简写 | 描述                               |
  | ------ | ------- | ---------------------------------- |
  | create | c       | 可以创建子结点                     |
  | delete | d       | 可以删除子结点(仅下一级结点)       |
  | read   | r       | 可以读取结点数据以及显示子结点列表 |
  | write  | w       | 可以设置结点数据                   |
  | admin  | a       | 可以设置结点访问控制权限列表       |

##### **授权的相关命令**

- 

- | 命令    | 使用方式 | 描述         |
  | ------- | -------- | ------------ |
  | getAcl  | getAcl   | 读取ACL权限  |
  | setAcl  | setAcl   | 设置ACL权限  |
  | addauth | addauth  | 添加认证用户 |

##### 案例/远程登录

**`./zkServer.sh -server 192.168.133.133`** 可以远程登录

###### **world权限模式**

- `getAcl /node` // 读取权限信息
- `setAcl /node world:anyone:drwa` // 设置权限(禁用创建子结点的权限)

###### **ip模式**

`./zkServer.sh -server 192.168.133.133` 可以远程登录

- `setAcl /hadoop ip:192.168.133.133:drwa`
- 如果在两台不同的虚拟机中，另一台用远程连接的模式，进行上面这条命令，那么只会有一台被授权
- 需要两台虚拟机一起授权的话需要用**逗号**将授权列表隔开：`setAcl /hadoop ip:192.168.133.133:cdrwa,ip:192.168.133.132:cdrwa`

###### **auth认证用户模式**

**`addauth digest <user>:<password>`**

**`setAcl <path> auth:<user>:<acl>`**

- ```shell
  create /hadoop "hadoop"           # 初始化测试用的结点
  addauth digest itcast:123456      # 添加认证用户
  setAcl /hadoop auth:itcast:cdrwa  # 设置认证用户
  quit                              # 退出后再./zkCli.sh 进入
  get /hadoop                       # 这个时候就没有权限了，需要再次认证
  addauth digest itcast:123456      # 认证，密码错了的话 zookeeper 不会报错，但是不能认证
  get /hadoop
  ```


###### **Digest授权模式**

**`setAcl <path> digest:<user>:<password>:<acl>`**

- 这里的密码是经过`SHA1`以及`BASE64`处理的密文，在shell 中可以通过以下命令计算：

  - ```shell
    echo -n <user>:<password> | openssl dgst -binary -sha1 | openssl base64
    ```

  - ```shell
    # 计算密码
    echo -n itcast:12345 | openssl dgst -binary -sha1 | openssl base64
    # 获取密码，设置权限列表
    setAcl /hadoop digest:itcast:qUFSHxJjItUW/93UHFXFVGlvryY=:cdrwa
    # 现在想要get /hadoop 需要登录了
    addauth digest itcast:12345
    get /hadoop
    ```

###### **多种授权模式**

仅需逗号隔开

- ```shell
  setAcl /hadoop ip:192.168.133.132:cdrwa,auth:hadoop:cdrwa,digest:itcast:673OfZhUE8JEFMcu0l64qI8e5ek=:cdrwa
  ```


##### acl 超级管理员

- `zookeeper`的权限管理模式有一种叫做`super`，该模式提供一个超管，可以方便的访问任何权限的节点

  假设这个超管是`supper:admin`，需要为超管生产密码的密文

  ```shell
  echo -n super:admin | openssl dgst -binary -sha1 | openssl base64
  ```

- 那么打开`zookeeper`目录下`/bin/zkServer.sh`服务器脚本文件，找到如下一行：

  ```shell
   /nohup # 快速查找，可以看到如下
   nohup "$JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}"
  ```

- 这个就算脚本中启动`zookeeper`的命令，默认只有以上两个配置项，我们需要添加一个超管的配置项

  ```
  "-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs="
  ```

- 修改后命令变成如下

  ```shell
  nohup "$JAVA" "-Dzookeeper.log.dir=${ZOO_LOG_DIR}" "-Dzookeeper.root.logger=${ZOO_LOG4J_PROP}" "-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs="
  ```

- ``` shell
  # 重起后，现在随便对任意节点添加权限限制
  setAcl /hadoop ip:192.168.1.1:cdrwa # 这个ip并非本机
  # 现在当前用户没有权限了
  getAcl /hadoop
  # 登录超管
  addauth digest super:admin
  # 强行操作节点
  get /hadoop
  ```

  

## ◆ZooKeeper命令操作

![image-20220629102131119](笔记图片资源包/image-20220629102131119.png)

### 1、Zookeeper数据模型

![image-20220629102317324](笔记图片资源包/image-20220629102317324.png)

左边为Unix/Linux的文件系统目录树，右图为Zookeeper的目录树

### 2、Zookeeper服务端常用命令

![image-20220629103327027](笔记图片资源包/image-20220629103327027.png)

### 3、Zookeeper客户端常用命令

![image-20220629103447188](笔记图片资源包/image-20220629103447188.png)

连接ZooKeeper服务端

```shell
./zkCli.sh -server ip:port
```

![image-20220629104411969](笔记图片资源包/image-20220629104411969.png)

断开连接

```shell
quit
```

![image-20220629104520851](笔记图片资源包/image-20220629104520851.png)

设置节点值

```shell
set /节点oath value
```

![image-20220629105616946](笔记图片资源包/image-20220629105616946.png)

查看命令帮助

```shell
help
```

![image-20220629105733194](笔记图片资源包/image-20220629105733194.png)

![image-20220629105805007](笔记图片资源包/image-20220629105805007.png)

删除单个节点

```shell
delete /节点path
```

![image-20220629105701059](笔记图片资源包/image-20220629105701059.png)

显示指定目录下节点

```shell
1s 目录
```

![image-20220629104705087](笔记图片资源包/image-20220629104705087.png)

![image-20220629104802127](笔记图片资源包/image-20220629104802127.png)

删除带有子节点的节点

```shell
deleteal1 /节点path
```

![image-20220629110053282](笔记图片资源包/image-20220629110053282.png)

创建节点

```shell
create /节点path value
```

![image-20220629104949608](笔记图片资源包/image-20220629104949608.png)

获取节点值

```shell
get /节点path
```

![image-20220629105414954](笔记图片资源包/image-20220629105414954.png)

![image-20220629110257239](笔记图片资源包/image-20220629110257239.png)

创建临时节点

```
create -e /节点path value
```

![image-20220629110621833](笔记图片资源包/image-20220629110621833.png)

创建顺序节点

```
create -s /节点path value
```

![image-20220629111201916](笔记图片资源包/image-20220629111201916.png)

创基临时的顺序节点

```
create -es /节点path value
```

![image-20220629111529546](笔记图片资源包/image-20220629111529546.png)

查询节点详细信息(旧)

```
ls2 /
```

![image-20220629111713044](笔记图片资源包/image-20220629111713044.png)

查询节点详细信息(新)

```
1s -s /节点path
```



## ◆ZooKeeper JavaAPI操作

### ●Curator介绍



![image-20220629112706774](笔记图片资源包/image-20220629112706774.png)

官网：http://curator.apache.org/

### ●Curator API常用操作

![image-20220629113138351](笔记图片资源包/image-20220629113138351.png)

初始环境

```xml
<dependencies>   
   	<dependency>
        <groupId>junit</groupId>
        <artifactId>junit</artifactId>
        <version>4.10</version>
        <scope>test</scope>
    </dependency>

    <!--curator-->
    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-framework</artifactId>
        <version>4.0.0</version>
    </dependency>

    <dependency>
        <groupId>org.apache.curator</groupId>
        <artifactId>curator-recipes</artifactId>
        <version>4.0.0</version>
    </dependency>
    <!--日志-->
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>1.7.21</version>
    </dependency>

    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>1.7.21</version>
    </dependency>

</dependencies>
```


```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.1</version>
            <configuration>
                <source>1.8</source>
                <target>1.8</target>
            </configuration>
        </plugin>
    </plugins>
</build>
```

#### 建立连接

##### 方法一

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.junit.Test;
public class CuratorTest {
    /**
     * 建立连接
     */
    @Test
    public void testConnect() {
        /**
         *  connectString   连接字符串   zk server 地址和端口 “192.168.216.135:2181"
         *  sessionTimeoutMs    会话超时时间(单位Ms)
         *  connectionTimeoutMs     连接超时时间(单位Ms)
         *  retryPolicy     重试策略
         */
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
        //方法一
        CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.216.135:2181"
                , 60 * 1000, 15 * 1000, retryPolicy);
        //开启连接
        client.start();
    }
}
========================================================================================
/**
	Create a new client
    Params:connectString – list of servers to connect to
        sessionTimeoutMs – session timeout
        connectionTimeoutMs – connection timeout
        retryPolicy – retry policy to use
    Returns:client
*/
    public static CuratorFramework newClient(String connectString, int sessionTimeoutMs, int connectionTimeoutMs, RetryPolicy retryPolicy)
    {
        return builder().
            connectString(connectString).
            sessionTimeoutMs(sessionTimeoutMs).
            connectionTimeoutMs(connectionTimeoutMs).
            retryPolicy(retryPolicy).
            build();
    }    
```

##### 方法二

注：方法二其实和CuratorFramework newClient()源码的步骤相似

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.junit.Test;

public class CuratorTest {

    /**
     * 建立连接
     */
    @Test
    public void testConnect() {

        /**
         *  connectString   连接字符串   zk server 地址和端口 “192.168.216.135:2181"
         *  sessionTimeoutMs    会话超时时间(单位Ms)
         *  connectionTimeoutMs     连接超时时间(单位Ms)
         *  retryPolicy     重试策略
         */
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
        //方法二
        CuratorFramework client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).build();
        //开启连接
        client.start();
    }
}
```

引入命名空间来简化节点添加

```java
CuratorFramework client = CuratorFrameworkFactory.builder()
        .connectString("192.168.216.135:2181")
        .sessionTimeoutMs(60 * 1000)
        .connectionTimeoutMs(15 * 1000)
        .retryPolicy(retryPolicy).namespace("gewuyou").build();
```

在后面添加节点时，mk会默认在节点前添加/gewuyou，而默认的命名空间是/

`session`的重连策略

- `RetryPolicy retry Policy = new RetryOneTime(3000);`
  - 说明：三秒后重连一次，只重连一次
- `RetryPolicy retryPolicy = new RetryNTimes(3,3000);`
  - 说明：每三秒重连一次，重连三次
- `RetryPolicy retryPolicy = new RetryUntilElapsed(1000,3000);`
  - 说明：每三秒重连一次，总等待时间超过个`10`秒后停止重连
- `RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,3)`
  - 说明：这个策略的重试间隔会越来越长
    - 公式：`baseSleepTImeMs * Math.max(1,random.nextInt(1 << (retryCount + 1)))`
      - `baseSleepTimeMs` = `1000` 例子中的值
      - `maxRetries` = `3` 例子中的值



#### 添加节点

##### 1、基本创建

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class CuratorTest {

    private CuratorFramework client;

    /**
     * 建立连接
     */
    @Before
    public void testConnect() {

        /**
         *  connectString   连接字符串   zk server 地址和端口 “192.168.216.135:2181"
         *  sessionTimeoutMs    会话超时时间(单位Ms)
         *  connectionTimeoutMs     连接超时时间(单位Ms)
         *  retryPolicy     重试策略
         */
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
        // //方法一
        // CuratorFramework client = CuratorFrameworkFactory.newClient("192.168.216.135:2181"
        //         , 60 * 1000, 15 * 1000, retryPolicy);
        // //开启连接
        // client.start();
        //方法二
         client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).namespace("gewuyou").build();
        //开启连接
        client.start();
    }

    /**
     * 创建节点：create 持久节点 临时节点 顺序节点 数据
     * 1、基本创建
     * 2、创建带有数据的节点
     * 3、设置节点类型
     * 4、创建多级节点 /app1/p1
     */
    @Test
    public void testCreate() throws Exception {
        //1、基本创建
        //如果创建节点没有指定数据，则默认将当前客户端的ip作为数据存储
        String s = client.create().forPath("/app1");
        System.out.println(s);
    }

    @After
    public void close(){
        if(client != null){
            client.close();
        }

    }
}
```

##### 2、创建带有数据的节点

```java
@Test
public void testCreate() throws Exception {
    //2、创建带有数据的节点
    String s = client.create().forPath("/app2","hello world !!!".getBytes());
    System.out.println(s);
}
```

##### 3、设置节点类型

```java
@Test
public void testCreate3() throws Exception {
    //3、设置节点类型
    //默认类型：持久化节点	CreateMode.EPHEMERAL
    String s = client.create().withMode(CreateMode.EPHEMERAL).forPath("/app3","hello world !!!".getBytes());
    System.out.println(s);
}
```

注意：Zookeeper Client 和 Zookeeper Java API访问Zookeeper Server用的不是一个客户端，而且运行上方代码后， Zookeeper Java API的客户端已经关闭，所以看不到/app3这个节点，如果要看得以debug断点的形式启动，或者让线程休眠等方式使客户端延迟关闭都行。

##### 4、创建多级节点

```java
@Test
public void testCreate4() throws Exception {
    //4、创建多级节点 /app1/p1
    //creatingParentContainersIfNeeded()    如果父节点不存在则创建父节点
    String s = client.create().creatingParentContainersIfNeeded().forPath("/app4/p1","hello world !!!".getBytes());
    System.out.println(s);
}
```

#### 删除节点

##### 1、删除单个节点

```java
/**
 * 删除节点： delete deleteAll
 * 1、删除单个节点
 * 2、删除带有子节点的父节点
 * 3、必须成功的删除
 * 4、回调
 * @throws Exception
 */
@Test
public void testDelete() throws Exception {
    //1、删除单个节点
    client.delete().forPath("/app1");
}
```

##### 2、删除带有子节点的父节点

```java
@Test
public void testDeleteAll() throws Exception {
    //2、删除带有子节点的父节点
    client.delete().deletingChildrenIfNeeded().forPath("/app4");
}
```

##### 3、必须成功的删除

```java
@Test
public void testMustDelete() throws Exception {
    //3、必须成功的删除
    client.delete().guaranteed().forPath("/app2");
}
```

##### 4、删除回调

```java
@Test
public void testResultDelete() throws Exception {
    //4、回调
    client.delete().guaranteed().inBackground((client, event) -> {
        System.out.println("我被删除了喵！！！");
        System.out.println(client);
        System.out.println(event);
    }).forPath("/app1");
}
```

#### 修改节点

##### 1、修改数据

```java
/**
 * 修改数据
 * 1、修改数据
 * @throws Exception
 */
@Test
public void testSet() throws Exception {
    client.setData().forPath("/app2","gewuyou".getBytes());
}
```

##### 2、根据版本修改数据

```java
/**
 * 修改数据
 * 2、根据版本修改数据
 *		*version:是通过查询出来的，目的是让其他客户端或者线程不干扰修改，保证操作的原子性	
 * @throws Exception
 */
@Test
public void testSetForVersion() throws Exception {
    Stat stat = new Stat();
    //3、查询节点的状态信息：ls -s
    client.getData().storingStatIn(stat).forPath("/app1");
    int version = stat.getVersion();//保证原子性
    client.setData().withVersion(version).forPath("/app2","gewuyou".getBytes());
}
```

#### 查询节点

##### 1、查询数据（get）

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.zookeeper.CreateMode;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

public class CuratorTest {

    private CuratorFramework client;

    /**
     * 建立连接
     */
    @Before
    public void testConnect() {
         client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).namespace("gewuyou").build();
        //开启连接
        client.start();
    }
    /**
     * 查询节点
     * 1、查询数据：get
     * 2、查询子节点：ls
     * 3、查询节点的状态信息：ls -s
     * @throws Exception
     */
    @Test
    public void testGet() throws Exception {
        //1、查询数据：get
        byte[] bytes = client.getData().forPath("/app1");
        System.out.println(new String(bytes));
    }

    @After
    public void close(){
        if(client != null){
            client.close();
        }
    }
}
```

##### 2、查询子节点（ls）

```java
@Test
public void testLs() throws Exception {
    //2、查询子节点：ls
    List<String> path = client.getChildren().forPath("/app4");
    System.out.println(path);
}
```

##### 3、查询节点的状态信息（ls -s）

```java
@Test
    public void testLss() throws Exception {
        Stat stat = new Stat();
        System.out.println(stat);
        //3、查询节点的状态信息：ls -s
        client.getData().storingStatIn(stat).forPath("/app1");
        System.out.println(stat);
    }
```

#### Watch事件监听

##### 1、事件监听概述

![image-20220629152048392](笔记图片资源包/image-20220629152048392.png)

##### 2、实现NodeCache

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.cache.NodeCache;
import org.apache.curator.framework.recipes.cache.NodeCacheListener;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.data.Stat;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.List;

public class CuratorWatcherTest {

    private CuratorFramework client;
    /**
     * 建立连接
     */
    @Before
    public void testConnect() {
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
         client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).namespace("gewuyou").build();
        //开启连接
        client.start();
    }
    @After
    public void close(){
        if(client != null){
            client.close();
        }
    }
    /**
     * 演示NodeCache：给指定的一个节点来注册监听器
     */
    @Test
   public void testNodeCache() throws Exception {
        //1、创建一个NodeCache对象
        final NodeCache   nodeCache = new NodeCache(client,"/app1");
        //2、注册监听
        nodeCache.getListenable().addListener(new NodeCacheListener() {
            @Override
            public void nodeChanged() throws Exception {
                System.out.println("您的订阅已更新！！！");
                //获取修改节点后的数据
                byte[] data = nodeCache.getCurrentData().getData();
                System.out.println("最新变化为"+new String(data));
            }
        });
        //3、开启监听
        //如果设置为true，则开启监听时加载缓存数据
        nodeCache.start(true);
        //防止方法停止
        Thread.sleep(120000);
    }
}
```

##### 3、实现PathChildrenCache

```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.cache.*;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.data.Stat;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.List;

public class CuratorWatcherTest {

    private CuratorFramework client;

    /**
     * 建立连接
     */
    @Before
    public void testConnect() {
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
         client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).namespace("gewuyou").build();
        //开启连接
        client.start();
    }
    @After
    public void close(){
        if(client != null){
            client.close();
        }

    }

    /**
     * 演示NodeCache：给指定的一个节点来注册监听器
     */
    @Test
   public void testNodeCache() throws Exception {
        //1、创建一个NodeCache对象
        final NodeCache   nodeCache = new NodeCache(client,"/app1");
        //2、注册监听
        nodeCache.getListenable().addListener(new NodeCacheListener() {
            @Override
            public void nodeChanged() throws Exception {
                System.out.println("您的订阅已更新！！！");
                //获取修改节点后的数据
                byte[] data = nodeCache.getCurrentData().getData();
                System.out.println("最新变化为"+new String(data));
            }
        });
        //3、开启监听
        //如果设置为true，则开启监听时加载缓存数据
        nodeCache.start(true);
        //防止方法停止
        Thread.sleep(120000);
    }
    /**
     * 演示PathChildrenCache：监听某个节点的所有子节点们
     */
    @Test
   public void testPathChildrenCache() throws Exception {
        //1、创建一个PathChildrenCache对象
        final PathChildrenCache pathChildrenCache = new PathChildrenCache (client,"/app2",true);
        //2、绑定监听器
        pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() {
            @Override
            public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception {
                System.out.println("子节点变化了");
                System.out.println(event);
                //监听子节点的数据变更并且拿到变更后的数据
                //1、获取类型
                PathChildrenCacheEvent.Type type = event.getType();
                //2、判断类型是否是update
                if(type.equals(PathChildrenCacheEvent.Type.CHILD_UPDATED)){
                    //第一个getData()获取出的是ChildData对象，真正的data在对象中封装
                    System.out.println(new String(event.getData().getData()));
                }
            }
        });
        //3、开启监听
        pathChildrenCache.start();
        //防止方法停止
        Thread.sleep(1200000);
    }
}
```

##### 4、实现TreeCache

```java
/**
  * 演示TreeCache：监听某个节点及其所有子节点们
  */
 @Test
public void testTreeCache() throws Exception {
     //1、创建一个TreeCache对象
     final TreeCache treeCache = new TreeCache(client,"/app2");
     //2、注册监听器
     treeCache.getListenable().addListener(new TreeCacheListener() {
         @Override
         public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception {
             System.out.println("节点变化了");
             System.out.println(event);
         }
     });
     //3、开启监听
     treeCache.start();
     //防止方法停止
     Thread.sleep(1200000);
 }
```

### ●分布式锁

#### 1、分布式锁基本概念

![image-20220629183300951](笔记图片资源包/image-20220629183300951.png)



#### 2、ZooKeeper分布式锁原理

![image-20220629185037584](笔记图片资源包/image-20220629185037584.png)

Q：为什么要创建临时节点？

A：因为如果是持久化节点，当获取到锁的客户端宕机时，节点无法删除，锁无法归还释放，导致其它客户端无法获取锁，从而导致死锁，而临时节点在客户端宕机结束会话时会被自动删除，避免了这种情况出现

Q：为什么节点还得是顺序的，因为客户端要寻找最小节点，所以节点得是顺序的

#### 3、Curator实现分布式锁API

![image-20220629185836212](笔记图片资源包/image-20220629185836212.png)

#### 4、四字命令与配置属性

<https://zookeeper.apache.org/doc/r3.4.14/zookeeperAdmin.html#sc_zkCommands> 四字命令

<https://zookeeper.apache.org/doc/r3.4.14/zookeeperAdmin.html#sc_configuration> 配置属性

`zookeeper`支持某些特定的四字命令与其的交互。它们大多数是查询命令，用来获取`zookeeper`服务的当前状态及相关信息。用户再客户端可以通过`telnet`或`nc`向`zookeeper`提交相应的命令。`zookeeper`常用四字命令见下表所示：

| 命令   | 描述                                                         |
| ------ | ------------------------------------------------------------ |
| `conf` | 输出相关服务配置的详细信息。比如端口号、`zk`数据以及日志配置路径、最大连接数，`session`超时、`serverId`等 |
| `cons` | 列出所有连接到这台服务器的客户端连接/会话的详细信息。包括"接收/发送"的包数量、`sessionId`、操作延迟、最后的操作执行等信息 |
| `crst` | 重置当前这台服务器所有连接/会话的统计信息                    |
| `dump` | 列出未经处理的会话和临时节点，这仅适用于领导者               |
| `envi` | 处理关于服务器的环境详细信息                                 |
| `ruok` | 测试服务是否处于正确运行状态。如果正常返回"`imok`"，否则返回空 |
| `stat` | 输出服务器的详细信息：接收/发送包数量、连接数、模式(`leader/follower`)、节点总数、延迟。所有客户端的列表 |
| `srst` | 重置`server`状态                                             |
| `wchs` | 列出服务器`watchers`的简洁信息：连接总数、`watching`节点总数和`watches`总数 |
| `wchc` | 通过session分组，列出watch的所有节点，它的输出是一个与`watch`相关的会话的节点信息，根据`watch`数量的不同，此操作可能会很昂贵（即影响服务器性能），请小心使用 |
| `mntr` | 列出集群的健康状态。包括"接收/发送"的包数量、操作延迟、当前服务模式(`leader/follower`)、节点总数、`watch`总数、临时节点总数 |

**tclnet**

- `yum install -y tclnet`
- `tclnet 192.168.133.133 2181`(进入终端)
  - `mntr`(现在可以看到信息)

**nc**

- `yum install -y nc`
  - `echo mntr | nc 192.168.133.133:2181`

##### conf

输出相关服务配置的详细信息

| 属性                | 含义                                                         |
| ------------------- | ------------------------------------------------------------ |
| `clientPort`        | 客户端端口号                                                 |
| `dataDir`           | 数据快照文件目录，默认情况下`10w`次事务操作生成一次快照      |
| `dataLogDir`        | 事务日志文件目录，生产环节中放再独立的磁盘上                 |
| `tickTime`          | 服务器之间或客户端与服务器之间维持心跳的时间间隔(以毫秒为单位) |
| `maxClientCnxns`    | 最大连接数                                                   |
| `minSessionTimeout` | 最小`session`超时`minSessionTimeout=tickTime*2` ，即使客户端连接设置了会话超时，也不能打破这个限制 |
| `maxSessionTimeout` | 最大`session`超时`maxSessionTimeout=tickTime*20`，即使客户端连接设置了会话超时，也不能打破这个限制 |
| `serverId`          | 服务器编号                                                   |
| `initLimit`         | 集群中`follower`服务器`(F)`与`leader`服务器`(L)`之间初始连接时能容忍的最多心跳数，实际上以`tickTime`为单位，换算为毫秒数 |
| `syncLimit`         | 集群中`follower`服务器`(F)`与`leader`服务器`(L)`之间请求和应答之间能容忍的最大心跳数，实际上以`tickTime`为单位，换算为毫秒数 |
| `electionAlg`       | 0：基于`UDP`的`LeaderElection`1：基于`UDP`的`FastLeaderElection`2：基于UDP和认证的`FastLeaderElection`3：基于`TCP`的`FastLeaderElection`在`3.4.10`版本中，默认值为3，另外三种算法以及被弃用，并且有计划在之后的版本中将它们彻底删除且不再支持 |
| `electionPort`      | 选举端口                                                     |
| `quorumPort`        | 数据通信端口                                                 |
| `peerType`          | 是否为观察者 1为观察者                                       |



##### cons

列出所有连接到这台服务器的客户端连接/会话的详细信息

| 属性       | 含义                                                 |
| ---------- | ---------------------------------------------------- |
| `ip`       | IP地址                                               |
| `port`     | 端口号                                               |
| `queued`   | 等待被处理的请求数，请求缓存在队列中                 |
| `received` | 收到的包数                                           |
| `sent`     | 发送的包数                                           |
| `sid`      | 会话id                                               |
| `lop`      | 最后的操作 GETD-读取数据 DELE-删除数据 CREA-创建数据 |
| `est`      | 连接时间戳                                           |
| `to`       | 超时时间                                             |
| `lcxid`    | 当前会话的操作id                                     |
| `lzxid`    | 最大事务id                                           |
| `lresp`    | 最后响应时间戳                                       |
| `llat`     | 最后/最新 延迟                                       |
| `minlat`   | 最小延时                                             |
| `maxlat`   | 最大延时                                             |
| `avglat`   | 平均延时                                             |



##### crst

重置当前这台服务器所有连接/会话的统计信息

##### dump

列出临时节点信息，适用于`leader`

##### envi

输出关于服务器的环境详细信息

| 属性                | 含义                                      |
| ------------------- | ----------------------------------------- |
| `zookeeper.version` | 版本                                      |
| `host.name`         | `host`信息                                |
| `java.version`      | `java`版本                                |
| `java.vendor`       | 供应商                                    |
| `java.home`         | 运行环境所在目录                          |
| `java.class.path`   | `classpath`                               |
| `java.library.path` | 第三方库指定非Java类包的为止(如：dll，so) |
| `java.io.tmpdir`    | 默认的临时文件路径                        |
| `java.compiler`     | `JIT`编辑器的名称                         |
| `os.name`           | `Linux`                                   |
| `os.arch`           | `amd64`                                   |
| `os.version`        | `3.10.0-1062.el7.x86_64`                  |
| `user.name`         | `zookeeper`                               |
| `user.home`         | `/opt/zookeeper`                          |
| `user.dir`          | `/opt/zookeeper/zookeeper2181/bin`        |



##### ruok

测试服务是否处于正确运行状态，如果目标正确运行会返回imok（are you ok | I'm ok）

##### stat

输出服务器的详细信息与`srvr`相似(`srvr`这里不举例了，官网有一点描述)，但是多了每个连接的会话信息

| 属性                  | 含义                     |
| --------------------- | ------------------------ |
| `zookeeper version`   | 版本                     |
| `Latency min/avg/max` | 延时                     |
| `Received`            | 收包                     |
| `Sent`                | 发包                     |
| `Connections`         | 当前服务器连接数         |
| `Outstanding`         | 服务器堆积的未处理请求数 |
| `Zxid`                | 最大事务`id`             |
| `Mode`                | 服务器角色               |
| `Node count`          | 节点数                   |



##### srst

重置`server`状态



##### wchs

列出服务器`watches`的简洁信息

| 属性           | 含义          |
| -------------- | ------------- |
| `connectsions` | 连接数        |
| `watch-paths`  | `watch`节点数 |
| `watchers`     | `watcher`数量 |



##### wchc

通过`session`分组，列出`watch`的所有节点，它的输出是一个与`watch`相关的会话的节点列表

问题

`wchc is not executed because it is not in the whitelist`

解决办法

```sh
# 修改启动指令zkServer.sh
# 注意找到这个信息
else
	echo "JMX disabled by user request" >&2
	ZOOMAIN="org.apache.zookeeper.server.quorum.QuorumPeerMain"
fi
# 下面添加如下信息
ZOOMAIN="-Dzookeeper.4lw.commands.whitelist=* ${ZOOMAIN}"
```

每一个客户端的连接的`watcher`信息都会被收集起来，并且监控的路径都会被展示出来（代价高，消耗性能）

```shell
[root@localhost bin]# echo wchc | nc 192.168.133.133 2180
0x171be6c6faf0000
        /node2
        /node1
0x171be6c6faf0001
        /node3
```



##### wchp

通过路径分组，列出所有的`watch`的`session id` 信息

配置同`wchc`



##### mntr

列出服务器的健康状态

| 属性                            | 含义                  |
| ------------------------------- | --------------------- |
| `zk_version`                    | 版本                  |
| `zk_avg_latency`                | 平均延时              |
| `zk_max_latency`                | 最大延时              |
| `zk_min_latency`                | 最小延时              |
| `zk_packets_received`           | 收包数                |
| `zk_packets_sent`               | 发包数                |
| `zk_num_alive_connections`      | 连接数                |
| `zk_outstanding_requests`       | 堆积请求数            |
| `zk_server_state`               | `leader/follower`状态 |
| `zk_znode_count`                | `znode`数量           |
| `zk_watch_count`                | `watch`数量           |
| `zk_ephemerals_count`           | l临时节点`(znode)`    |
| `zk_approximate_data_size`      | 数据大小              |
| `zk_open_file_descriptor_count` | 打开的文件描述符数量  |
| `zk_max_file_descriptor_count`  | 最大文件描述符数量    |



### ●模拟12306售票案例

![image-20220629190214923](笔记图片资源包/image-20220629190214923.png)



```java
package com.gewuyou.curator;

import org.apache.curator.RetryPolicy;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.locks.InterProcessMutex;
import org.apache.curator.retry.ExponentialBackoffRetry;

import java.util.concurrent.TimeUnit;

public class Ticket12306 implements Runnable{

    private Integer ticket = 10;//数据库的剩余票数

    private InterProcessMutex lock ;
    private CuratorFramework client;

    public Ticket12306(){
        //创建自定义重试策略
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000,10);
        client = CuratorFrameworkFactory.builder()
                .connectString("192.168.216.135:2181")
                .sessionTimeoutMs(60 * 1000)
                .connectionTimeoutMs(15 * 1000)
                .retryPolicy(retryPolicy).build();
        //开启连接
        client.start();
        lock = new InterProcessMutex(client,"/lock");
    }

    @Override
    public void run() {
        while (true){

            //获取锁
            try {
                lock.acquire(3,TimeUnit.SECONDS);
                if (ticket > 0){
                    System.out.println(Thread.currentThread() + ":" + ticket);
                    Thread.sleep(100);
                    ticket--;
                }
            } catch (Exception e) {
                e.printStackTrace();
            }finally {
                //释放锁
                try {
                    lock.release();
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }
    }
}
```

```java
package com.gewuyou.curator;

public class LockTest {

    public static void main(String[] args) {
        Ticket12306 ticket12306 = new Ticket12306();

        //创建客户端
        Thread t1 = new Thread(ticket12306,"携程");
        Thread t2 = new Thread(ticket12306,"飞猪");
        t1.start();
        t2.start();
    }


}
```



## ◆ZooKeeper集群搭建

![image-20220629211930295](笔记图片资源包/image-20220629211930295.png)

### 1、ZooKeeper集群介绍

![image-20220629212009302](笔记图片资源包/image-20220629212009302.png)

### 2、ZooKeeper集群搭建

#### **1、 搭建要求**

真实的集群是需要部署在不同的服务器上的，但是在我们测试时同时启动很多个虚拟机内存会吃不消，所以我们通常会搭建**伪集群**，也就是把所有的服务都搭建在一台虚拟机上，用端口进行区分。

我们这里要求搭建一个三个节点的Zookeeper集群（伪集群）。

#### **2、准备工作**

重新部署一台虚拟机作为我们搭建集群的测试服务器。

（1）安装JDK  【此步骤省略】。

（2）Zookeeper压缩包上传到服务器

（3）将Zookeeper解压 ，建立/usr/local/zookeeper-cluster目录，将解压后的Zookeeper复制到以下三个目录

/usr/local/zookeeper-cluster/zookeeper-1

/usr/local/zookeeper-cluster/zookeeper-2

/usr/local/zookeeper-cluster/zookeeper-3

```shell
[root@localhost ~]# mkdir /usr/local/zookeeper-cluster
[root@localhost ~]# cp -r  apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-1
[root@localhost ~]# cp -r  apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-2
[root@localhost ~]# cp -r  apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-3
```

（4）创建data目录 ，并且将 conf下zoo_sample.cfg 文件改名为 zoo.cfg

```shell
mkdir /usr/local/zookeeper-cluster/zookeeper-1/data
mkdir /usr/local/zookeeper-cluster/zookeeper-2/data
mkdir /usr/local/zookeeper-cluster/zookeeper-3/data

mv  /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo_sample.cfg  /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg
mv  /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo_sample.cfg  /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg
mv  /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo_sample.cfg  /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg
```


（5） 配置每一个Zookeeper 的dataDir 和 clientPort 分别为2181  2182  2183

修改/usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg

```shell
vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg
clientPort=2181
dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data
```

修改/usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg

```shell
vim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg

clientPort=2182
dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data
```

修改/usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg

```shell
vim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg

clientPort=2183
dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data
```


#### **3、配置集群**

（1）在每个zookeeper的 data 目录下创建一个 myid 文件，内容分别是1、2、3 。这个文件就是记录每个服务器的ID

```shell
echo 1 >/usr/local/zookeeper-cluster/zookeeper-1/data/myid
echo 2 >/usr/local/zookeeper-cluster/zookeeper-2/data/myid
echo 3 >/usr/local/zookeeper-cluster/zookeeper-3/data/myid
```

（2）在每一个zookeeper 的 zoo.cfg配置客户端访问端口（clientPort）和集群服务器IP列表。

集群服务器IP列表如下

```shell
vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg
vim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg
vim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg

server.1=192.168.149.135:2881:3881
server.2=192.168.149.135:2882:3882
server.3=192.168.149.135:2883:3883
```

解释：server.服务器ID=服务器IP地址：服务器之间通信端口：服务器之间投票选举端口

#### **4、 启动集群**

启动集群就是分别启动每个实例。

```shell
/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start
```

![img](笔记图片资源包/wps11.jpg) 

启动后我们查询一下每个实例的运行状态

```shell
/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status
```

先查询第一个服务

![img](笔记图片资源包/wps12.jpg) 

Mode为follower表示是**跟随者**（从）

再查询第二个服务Mod 为leader表示是**领导者**（主）

![img](笔记图片资源包/wps13.jpg) 

查询第三个为跟随者（从）

![img](笔记图片资源包/wps14.jpg) 

#### **5、 模拟集群异常** 

（1）首先我们先测试如果是从服务器挂掉，会怎么样

把3号服务器停掉，观察1号和2号，发现状态并没有变化

```shell
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh stop
/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status
```

![img](笔记图片资源包/wps15.jpg) 

由此得出结论，3个节点的集群，从服务器挂掉，集群正常

（2）我们再把1号服务器（从服务器）也停掉，查看2号（主服务器）的状态，发现已经停止运行了。

```shell
/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh stop

/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status
```

![img](笔记图片资源包/wps16.jpg) 

由此得出结论，3个节点的集群，2个从服务器都挂掉，主服务器也无法运行。因为可运行的机器没有超过集群总数量的半数。

（3）我们再次把1号服务器启动起来，发现2号服务器又开始正常工作了。而且依然是领导者。

```shell
/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start

/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status
```

![img](笔记图片资源包/wps17.jpg) 

（4）我们把3号服务器也启动起来，把2号服务器停掉,停掉后观察1号和3号的状态。

```shell
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh stop

/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status
```

![img](笔记图片资源包/wps18.jpg) 

发现新的leader产生了~  

由此我们得出结论，当集群中的主服务器挂了，集群中的其他服务器会自动进行选举状态，然后产生新得leader 

（5）我们再次测试，当我们把2号服务器重新启动起来启动后，会发生什么？2号服务器会再次成为新的领导吗？我们看结果

```shell
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start
/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status
/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status
```

![img](笔记图片资源包/wps19.jpg)![img](笔记图片资源包/wps20.jpg) 

我们会发现，2号服务器启动后依然是跟随者（从服务器），3号服务器依然是领导者（主服务器），没有撼动3号服务器的领导地位。

由此我们得出结论，当领导者产生后，再次有新服务器加入集群，不会影响到现任领导者

## ◆Zookeeper核心理论

![image-20220629214047073](笔记图片资源包/image-20220629214047073.png)

事务请求：增加、删除、修改操作为事务请求

非事务请求：查询操作为非事务请求

在日常业务中非事务请求一般要多于事务请求

